{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline System "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch  import Elasticsearch\n",
    "from typing import Dict, List, Optional\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/subankankarunakaran/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "2021-11-20 16:55:50 [INFO] loader: Loading faiss with AVX2 support.\n",
      "2021-11-20 16:55:50 [INFO] loader: Could not load library with AVX2 support due to:\n",
      "ModuleNotFoundError(\"No module named 'faiss.swigfaiss_avx2'\")\n",
      "2021-11-20 16:55:50 [INFO] loader: Loading faiss.\n",
      "2021-11-20 16:55:50 [INFO] loader: Successfully loaded faiss.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "from pygaggle.rerank.base import Query, Text\n",
    "from pygaggle.rerank.transformer import MonoBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_NAME = \"cast_base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CAsT_base():\n",
    "    def __init__(self, context_responses: int = 0, reranking: bool = False,remove_stopwords: bool = True) -> None:\n",
    "        self.INDEX_NAME = \"cast_base\"\n",
    "        self.es = Elasticsearch()\n",
    "        self.queries = []\n",
    "        self.responses = []\n",
    "        self.reranking = reranking\n",
    "        self.reranker = MonoBERT() if reranking else None \n",
    "        self.context_responses = context_responses\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "\n",
    "    def clear_context(self, clear_queries: bool = True, clear_responses: bool = True):\n",
    "        if clear_queries:\n",
    "            self.queries = []\n",
    "        if clear_responses:\n",
    "            self.responses = []\n",
    " \n",
    "    def listToString(self,s: List): \n",
    "        # initialize an empty string\n",
    "        str1 = \" \" \n",
    "        \n",
    "        # return string  \n",
    "        return (str1.join(s))\n",
    "\n",
    "    def query(self, q: str) -> str:\n",
    "        \"\"\" \n",
    "        Preprocessing query and scoring using bm25\n",
    "        \"\"\"\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        \n",
    "        if self.remove_stopwords:\n",
    "            tokens = word_tokenize(q)\n",
    "            q_list = []\n",
    "            for w in tokens:\n",
    "                if w not in stop_words:\n",
    "                    q_list.append(w)\n",
    "            q = self.listToString(q_list)\n",
    "            \n",
    "            \n",
    "        hits = es.search(\n",
    "            index=self.INDEX_NAME, q=q, _source=True, size=100\n",
    "        ).get(\"hits\", {}).get(\"hits\")\n",
    "        \n",
    "        \n",
    "        hits_cleaned = [{\n",
    "            \"passage\": hit.get(\"_source\", {}).get(\"passage\"),\n",
    "            \"_id\": \"MARCO_\" + hit.get(\"_id\") if hit.get(\"_source\").get(\n",
    "                    \"origin\") == \"msmarco\" else \"CAR_\" + hit.get(\"_id\"),\n",
    "            \"_score\": hit.get(\"_score\", \"FAILED\")} for hit in hits]\n",
    "        \n",
    "        if self.reranking:\n",
    "            print(\"RERANKING\")\n",
    "            texts = [Text(hit.get(\"passage\"), {\n",
    "                '_id': hit.get(\"_id\", \"FAILED\")}, 0) for hit in hits_cleaned]\n",
    "\n",
    "            reranked = self.reranker.rerank(Query(q), texts)\n",
    "            hits_cleaned = [{\n",
    "                \"passage\": hit.text,\n",
    "                \"_id\": hit.metadata[\"_id\"],\n",
    "                \"_score\": hit.score}\n",
    "                for hit in reranked]\n",
    "        \n",
    "        \n",
    "        if len(hits) > 0:\n",
    "            print(\"Query: \" + q)\n",
    "            return hits_cleaned[:1000]\n",
    "        else:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_queries(query_file: str, key: str, CAsT: object, run_id: str):\n",
    "    queries = json.load(open(query_file))\n",
    "    if queries[0].get(\"turn\", {})[0].get(key) is None:\n",
    "        raise KeyError(\"Provided key: \" + key +\n",
    "                       \"is not a valid key for queryfile\")\n",
    "    total_num = len(queries)\n",
    "    f = open(run_id + \".trec\", \"w\")\n",
    "\n",
    "    for i, topic in enumerate(queries):\n",
    "        print(\"Topic: {}/{}\".format(i+1, total_num))\n",
    "        CAsT.clear_context()\n",
    "        topic_id = topic.get(\"number\")\n",
    "        for turn in topic.get(\"turn\"):\n",
    "            turn_id = turn.get(\"number\")\n",
    "            hits = CAsT.query(turn.get(key))\n",
    "            for j, hit in enumerate(hits):\n",
    "                f.write(str(topic_id) + \"_\" + str(turn_id) + \"\\t\" + \"Q0\" + \"\\t\" + str(hit.get(\"_id\")) +\n",
    "                        \"\\t\" + str(j) + \"\\t\" + str(hit.get(\"_score\")) + \"\\t\" + str(run_id) + \"\\n\")\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"../eval/2020_automatic_evaluation_topics_v1.0.json\"\n",
    "key = \"raw_utterance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cast = CAsT_base(reranking=True,remove_stopwords=True)\n",
    "run_queries(path, key=key, CAsT=cast, run_id=\"Test\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a18693a6d3eba38831bb504b1cb5882ed71b52e6a1e7daf3d617604e8d5d56db"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
